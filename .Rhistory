token_secret<-"gHBcVW1q50U57refumM5cvfcMMeoWc8jMs54hDqgKfVui"
setup_twitter_oauth(key_api,secret_api,token_acces,token_secret)
1
2
3
4
5
6
7
install.packages(c("devtools", "rjson", "bit64", "httr"))
#RESTART R session!
library(devtools)
install_github("twitteR", username="geoffjentry")
library(twitteR)
install.packages("twitteR")
setup_twitter_oauth
2
3
4
5
6
7
install.packages(c("devtools", "rjson", "bit64", "httr"))
install.packages(c("devtools", "rjson", "bit64", "httr"))
library(twitteR)
library(tidyverse)
library(ggplot2)
library(tidytext)
library(stringr)
#<<<<<<< Updated upstream:twitter scraper.R
key_api<-"qU2NfkJlLgTvbVR73JvJxKWXJ"
secret_api<-"MM8sOrlo2n3LQVNpNqYtLEKjv7Aa7GBBDzPqNfDid1x9k6Xkxl"
token_acces<-"144713953-zjghANhbh4CYoiyCQq5a0MvCKKskY42P4Sa82SEn"
token_secret<-"gHBcVW1q50U57refumM5cvfcMMeoWc8jMs54hDqgKfVui"
setup_twitter_oauth(key_api,secret_api,token_acces,token_secret)
#Test search
tweets_ds<-searchTwitter('#DataScience',n=1000)
library(tidytext)
key_api<-"qU2NfkJlLgTvbVR73JvJxKWXJ"
secret_api<-"MM8sOrlo2n3LQVNpNqYtLEKjv7Aa7GBBDzPqNfDid1x9k6Xkxl"
token_acces<-"144713953-zjghANhbh4CYoiyCQq5a0MvCKKskY42P4Sa82SEn"
token_secret<-"gHBcVW1q50U57refumM5cvfcMMeoWc8jMs54hDqgKfVui"
setup_twitter_oauth(key_api,secret_api,token_acces,token_secret)
#Test search
tweets_ds<-searchTwitter('#DataScience',n=1000)
tweets.df4<-twListToDF(tweets_ds)
just_text<-tweets.df4$text%>%unlist()
pattern<-'(?<=#)[A-z]+'
just_hash<-str_extract_all(just_text,pattern)
just_hash_unlist<-just_hash%>%unlist()
word.df<-data.frame("word"=just_hash_unlist)
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
count_word_cut<-count_word%>%
filter(word_count>20)
ggplot(count_word_cut, mapping=aes(x=reorder(word,word_count),y=word_count))+
geom_bar(stat="identity")+
coord_flip() +
labs(title="Twitter Scrape for hashtags when #DataScience is Used",x="hashtags", y="count")
View(count_word_cut)
ggplot(count_word_cut, mapping=aes(x=reorder(word,word_count),y=word_count))+
geom_bar(stat="identity")+
coord_flip() +
labs(title="Twitter Scrape for hashtags when #DataScience is Used",x="hashtags", y="count")
install.packages("keyring")
keyring::key_set(key_api = "qU2NfkJlLgTvbVR73JvJxKWXJ",
secret_api = "MM8sOrlo2n3LQVNpNqYtLEKjv7Aa7GBBDzPqNfDid1x9k6Xkxl")
keyring::key_list()
keyring::key_set(key_api = "qU2NfkJlLgTvbVR73JvJxKWXJ")
keyring::key_list()
keyring::key_set(service="twitter", key_api = "qU2NfkJlLgTvbVR73JvJxKWXJ")
View(word.df)
View(word.df)
write.csv(word.df, "twitter_words.csv")
knitr::opts_chunk$set(echo = TRUE)
word.df1<-read.csv("data/twitter_words.csv")
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
summary(count_word)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
kable(count_word)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
head(count_word)
count_word_cut<-count_word%>%
filter(word_count>20)
ggplot(count_word_cut, mapping=aes(x=reorder(word,word_count),y=word_count))+
geom_bar(stat="identity")+
coord_flip() +
labs(title="Twitter Scrape for hashtags when #DataScience is Used",x="hashtags", y="count")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
head(count_word)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word))%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups=TRUE)%>%
arrange(word_count)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups="keep")%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups = 'drop')%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups = 'drop_last')%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups = 'drop_last')%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
count_word_cut<-count_word%>%
filter(word_count>20)
ggplot(count_word_cut, mapping=aes(x=reorder(word,word_count),y=word_count))+
geom_bar(stat="identity")+
coord_flip() +
labs(title="Twitter Scrape for hashtags when #DataScience is Used",x="hashtags", y="count")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
word.df<-read.csv("data/twitter_words.csv")
unique_word<-word.df%>%
group_by(word)%>%
summarize(word_count=n_distinct(word), .groups = 'drop_last')%>%
arrange(word_count)
count_word<-word.df%>%
filter(word!="DataScience")%>%
group_by(word)%>%
summarize(word_count=sum(!is.na(word)))%>%
arrange(desc(word_count))
knitr::kable(head(count_word))
count_word_cut<-count_word%>%
filter(word_count>20)
ggplot(count_word_cut, mapping=aes(x=reorder(word,word_count),y=word_count))+
geom_bar(stat="identity")+
coord_flip() +
labs(title="Twitter Scrape for hashtags when #DataScience is Used",x="hashtags", y="count")
Twitter is a social network founded in 2006, it has over 325 million members and serves as a barometer of public opinion.  Social media has played a fundamental role in the social activism of the 21st century.  Nevertheless, people share their opinions and connect in a wide range of areas including careers.  Is precisely this fact that motivated us to treat Twitter as a barometer for what hashtags people associate most commonly to Data Science.
include("get_db_connection.R")
source("get_db_connection.R")
source("R script segments for presentation/FUNCTIONS/get_db_connection.R")
job_corpus<-corpus(jobs_vector)
null_corpus<-corpus(null_vector)
dfmat_data<-dfm(job_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
job_corpus<-corpus(jobs_vector)
# connect to AWS DB
# prompt for input
mydb <- getDbConnection()
# Login to AWS
library(DBI)
library(tidyverse)
library(quanteda)
source("R script segments for presentation/FUNCTIONS/get_db_connection.R")
# connect to AWS DB
# prompt for input
mydb <- getDbConnection()
#load dbs
dbListTables(mydb)
df_data<-dbReadTable(mydb,"nyc_data_scrape")
df_null<-dbReadTable(mydb,"null_linkedin")
jobs_vector<-as.vector(df_data$description)
null_vector<-as.vector(df_null$description)
job_corpus<-corpus(jobs_vector)
null_corpus<-corpus(null_vector)
null_corpus<-corpus(null_vector)
dfmat_data<-dfm(job_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
dfmat_null<-dfm(null_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
data_sum<-colSums(dfmat_data)
data_words<-names(dfmat_data)
df_data<-rbind(data_words,data_sum)%>%
df_data[2,]
null_sum<-colSums(dfmat_null)
null_words<-names(dfmat_null)
df_null<-rbind(null_words,null_sum)
df_dat_null<-smartbind(df_data,df_null, fill=0)
column_names<-c("data","null")
test<-t(df_dat_null)
colnames(test)<-c("data","null")
df_long<-as.data.frame(test)
df_long$data<-as.numeric(df_long$data)
df_long$null<-as.numeric(df_long$null)
install.packages("gtools")
# Login to AWS
library(DBI)
library(tidyverse)
library(quanteda)
library(gtools)
# Import the DB Connection function
source("R script segments for presentation/FUNCTIONS/get_db_connection.R")
# connect to AWS DB
# prompt for input
mydb <- getDbConnection()
#load dbs
dbListTables(mydb)
df_data<-dbReadTable(mydb,"nyc_data_scrape")
df_null<-dbReadTable(mydb,"null_linkedin")
jobs_vector<-as.vector(df_data$description)
null_vector<-as.vector(df_null$description)
job_corpus<-corpus(jobs_vector)
null_corpus<-corpus(null_vector)
dfmat_data<-dfm(job_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
dfmat_null<-dfm(null_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
data_sum<-colSums(dfmat_data)
data_words<-names(dfmat_data)
df_data<-rbind(data_words,data_sum)%>%
df_data[2,]
null_sum<-colSums(dfmat_null)
null_words<-names(dfmat_null)
df_null<-rbind(null_words,null_sum)
df_dat_null<-smartbind(df_data,df_null, fill=0)
column_names<-c("data","null")
test<-t(df_dat_null)
colnames(test)<-c("data","null")
df_long<-as.data.frame(test)
df_long$data<-as.numeric(df_long$data)
df_long$null<-as.numeric(df_long$null)
test<-df_long%>%
mutate(delta=data-null)
test3<-rownames(df_long)
df_final<-cbind(test3,test$delta)
#set column names
colnames(df_final)<-c("word","delta")
#turn into data frame for graphing
df_final<-as.data.frame(df_final)
#arrange by descending delta
df_final1<-df_final%>%
arrange(desc(delta))
view(df_final1)
df_final1$delta<-as.integer(df_final1$delta)
view(gg_output)
#arrange by order and
output_df<-df_final1%>%arrange(desc(delta))
gg_output<-output_df%>%
filter(delta>60)
#remove "data" and "science" for clarity
gg_output<-gg_output[-c(1,2),]
#plot
ggplot(gg_output, aes(x=reorder(word,delta), delta))+
geom_bar(stat="identity")+
coord_flip()+
labs(title="Largest difference in count between a null scrape and a scrape for 'data science'")+
ylab("words")+
xlab("total difference")
data_words<-names(dfmat_data)
df_data<-rbind(data_words,data_sum)%>%
df_data[2,]
#load dbs
dbListTables(mydb)
df_data<-dbReadTable(mydb,"nyc_data_scrape")
View(df_data)
df_null<-dbReadTable(mydb,"null_linkedin")
jobs_vector<-as.vector(df_data$description)
null_vector<-as.vector(df_null$description)
job_corpus<-corpus(jobs_vector)
null_corpus<-corpus(null_vector)
dfmat_data<-dfm(job_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
View(dfmat_data)
dfmat_null<-dfm(null_corpus,
remove = stopwords("english"),
stem = FALSE, remove_punct = TRUE
)
data_sum<-colSums(dfmat_data)
data_words<-names(dfmat_data)
df_data<-rbind(data_words,data_sum)%>%
df_data[2,]
View(dfmat_data)
names(dfmat_data)
null_sum<-colSums(dfmat_null)
null_words<-names(dfmat_null)
